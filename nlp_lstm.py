# -*- coding: utf-8 -*-
"""NLP_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LxtHXh12JQ0koeiGNALZh1dFw33jA64F

<a href="https://colab.research.google.com/github/elnaggar96/ahmed-elnagar/blob/main/NLP_Projet_using_LSTM.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.simplefilter(action='ignore', category=Warning)
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from tqdm import tqdm
import seaborn as sns
sns.set_style("darkgrid")
import matplotlib.pyplot as plt
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import tensorflow as tf

#libraries for NLP
import re
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud,STOPWORDS
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, metrics, preprocessing, ensemble, model_selection, metrics
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all'
from tqdm import tqdm
from IPython.display import HTML
#!pip install chart_studio
import plotly
import plotly.subplots
import plotly.graph_objs as go
import chart_studio.plotly as py
import plotly.figure_factory as ff
from plotly.offline import iplot
plotly.offline.init_notebook_mode(connected=True)
import cufflinks
cufflinks.go_offline()
cufflinks.set_config_file(world_readable=True, theme='pearl')
import plotly.express as px
from collections import defaultdict
from tensorflow import keras
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import neighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV
from yellowbrick.classifier import ConfusionMatrix
from sklearn.model_selection import RandomizedSearchCV
import tensorflow as tf
from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout,Conv1D, Bidirectional, Dropout, Input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
import warnings
warnings.filterwarnings('ignore')
import os
import joblib
import ydata_profiling
from pycaret.classification import *

from google.colab import drive
drive.mount('/content/drive')

""" import our disaster tweet dataset"""

data = pd.read_csv("/content/train.csv")
Testdata =pd.read_csv('/content/test.csv')

"""Check head and info of the data

"""

data.head()

data.info()

data.shape

#Count of duplicate values
data.duplicated().sum()

data.describe()

data["target"].value_counts(normalize = True) #normalized value counts

def length_plot(data, name):
  length = [len(sentence.split()) for sentence in data]
  plt.hist(length)
  plt.title(name)
length_plot(data[data["target"]==0]["text"], "Not Disaster")
length_plot(data[data["target"]==1]["text"], "Disaster")

"""Missing data
AND Data in each class
"""

data.isna().sum()

percent_missing = data.isnull().sum() * 100 / len(data)
missing_value_df = pd.DataFrame({'column_name': data.columns,
                                 'percent_missing': percent_missing})
percent_missing
missing_value_df

""" separate the dependent and independent features"""

X = data["text"] # indpendent feature
y = data["target"] # dependent feature
y = np.array(y) # converting to array

"""Calculating the number of unique words present in the disaster tweets"""

def unq_words(sentence):
  unq_words_list = []
  for sent in tqdm(sentence):
    for word in sent.split():
      if word.lower() not in unq_words_list:
        unq_words_list.append(word.lower())
      else:
        pass
  return unq_words_list
unique_words = unq_words(X)
print("Total unique words present :",len(unique_words))

unique_words[:15]

"""words starting with ‚Äú#‚Äù"""

SYMBOL_1 = "#"
sym1_words = [word for word in unique_words if word.startswith(SYMBOL_1)]
len(sym1_words)

"""words starting with ‚Äú@‚Äù"""

SYMBOL_2 = "@"
sym2_words = [word for word in unique_words if word.startswith(SYMBOL_2)]
len(sym2_words)

"""remove the  urls present"""

def url_remover(text):
    url_patterns = re.compile(r'https?://S+|www.S+')
    return url_patterns.sub(r'', text)

# Tokenization of paragraphs/sentences
import nltk
nltk.download("all")

"""Top 15 locations of the data"""

data["location"].nunique()

data.location.value_counts()[:15].to_frame()

word_plot=data['location'].value_counts()[:15]
sns.barplot(x=word_plot,y=word_plot.index,palette='Dark2')
plt.title("Top 15 locations")
plt.xlabel("Count of locations")

"""Top 15 keyword in the data"""

data["keyword"].nunique()

data.keyword.value_counts()[:15].to_frame()

word_plot=data['keyword'].value_counts()[:15]
sns.barplot(x=word_plot,y=word_plot.index,palette='Dark2')
plt.title("Top 15 Keywords")
plt.xlabel("Count of Keywords")

"""Unique Words"""

X = data["text"] # indpendent feature
def uniq_words(sentence):
  uniq_words_list = []
  for sent in tqdm(sentence):
    for word in sent.split():
      if word.lower() not in uniq_words_list:
        uniq_words_list.append(word.lower())
      else:
        pass
  return uniq_words_list
unique_words = uniq_words(X)
print("Total unique words present :",len(unique_words))

unique_words[:15]

"""removed un necessary symbols stopwords"""

from nltk.stem import WordNetLemmatizer
wl = WordNetLemmatizer()
def preprocessing(text):
  tweets = []
  for sentence in tqdm(text):
    sentence = sentence.lower() # converting the words to lower case
    sentence =  url_remover(sentence) # removing the url from the sentence
    sentence = re.sub(r'@w+',  '', sentence).strip() # removing the words starts with "@"
    sentence = re.sub("[^a-zA-Z0-9 ']", "", sentence) # removing symbols
    sentence = sentence.split()
    sentence1 = [wl.lemmatize(word) for word in sentence if word not in set(stopwords.words("english"))] #lemmatization and stopwrds removal from tweets
    sentence1 = " ".join(sentence1)
    tweets.append(sentence1)
  return tweets
tweets = preprocessing(X)

from tensorflow.keras.layers import (Embedding,
                                     LSTM,
                                     Dense,
                                     Dropout,
                                     GlobalMaxPool1D,
                                     BatchNormalization)
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import one_hot

"""performing onehot encoding"""

VOC_SIZE = 30000
onehot_vector= [one_hot(words, VOC_SIZE) for words in tweets]
onehot_vector[110:120]

"""the word length each for each tweets."""

word_length = []
for i in onehot_vector:
  word_length.append(len(i))

len(word_length)

""" maximum word length"""

max(word_length)

""" make equal length sequences."""

SENTENCE_LENGTH = 15
embedded_docs = pad_sequences(onehot_vector, padding="post", maxlen=SENTENCE_LENGTH)
embedded_docs

"""model creation step. The first layer is a word embedding layer followed LSTM model"""

def model():
  VECTOR_FEATURES = 32
  lstm_model = Sequential()
  lstm_model.add(Embedding(VOC_SIZE,
                      VECTOR_FEATURES,
                      input_length=SENTENCE_LENGTH))
  lstm_model.add(LSTM(100, return_sequences = True))
  lstm_model.add(GlobalMaxPool1D())
  lstm_model.add(BatchNormalization())
  lstm_model.add(Dropout(0.5))
  lstm_model.add(Dense(10, activation="relu"))
  lstm_model.add(Dropout(0.25))
  lstm_model.add(Dense(1, activation = "sigmoid"))
  return lstm_model

"""Creating the model and getting the model summary"""

lstm_model = model()
lstm_model.compile(optimizer = "adam", loss = "binary_crossentropy", metrics = ["accuracy"])
lstm_model.summary()

"""Training the model"""

history = lstm_model.fit(embedded_docs, y, epochs=10, batch_size=32)

"""analyze our model by plotting the graph of model accuracy and loss

**for accuracy**
"""

plt.plot(history.history["accuracy"])
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Accuracy")

"""for loss"""

plt.plot(history.history["loss"])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss")

label_encoder_keyword = LabelEncoder()
label_encoder_location = LabelEncoder()
label_encoder_text = LabelEncoder()
data['keyword'] = label_encoder_keyword.fit_transform(data['keyword'])
data['location'] = label_encoder_location.fit_transform(data['location'])
data['text'] = label_encoder_text.fit_transform(data['text'])
data.head()

X = data.drop('id', axis = 1)
X = X.values
y = data['target']

scaler = StandardScaler()
X_standard = scaler.fit_transform(X)

X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

print("X_train.shape: ", X_train.shape[0])
print("y_train.shape: ", y_train.shape[0])
print("X_test.shape: ", X_test.shape[0])
print("y_test.shape: ", y_test.shape[0])

knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
knn_classifier.fit(X_train, y_train)
predictions = knn_classifier.predict(X_test)

classification_knn = (classification_report(y_test, predictions))
print(classification_knn)

cm = ConfusionMatrix(knn_classifier)
cm.fit(X_train, y_train)
cm.score(X_test, y_test)

logreg_classifier = LogisticRegression()
logreg_classifier.fit(X_train, y_train)
predictions = logreg_classifier.predict(X_test)

cm = ConfusionMatrix(logreg_classifier)
cm.fit(X_train, y_train)
cm.score(X_test, y_test)

classification_logistic_regression = (classification_report(y_test, predictions))
print(classification_logistic_regression)

min_split = np.array([2, 3, 4, 5, 6, 7])
max_nvl = np.array([3, 4, 5, 6, 7, 9, 11])
alg = ['entropy', 'gini']
values_grid = {'min_samples_split': min_split, 'max_depth': max_nvl, 'criterion': alg}

decision_tree = DecisionTreeClassifier()
gridDecisionTree = GridSearchCV(estimator = decision_tree, param_grid = values_grid, cv = 5)
gridDecisionTree.fit(X_train, y_train)

print('M√≠n Split: ', gridDecisionTree.best_estimator_.min_samples_split)
print('Max Nvl: ', gridDecisionTree.best_estimator_.max_depth)
print('Algorithm: ', gridDecisionTree.best_estimator_.criterion)
print('Score: ', gridDecisionTree.best_score_)

def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")

    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))
        print("Test Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")

tree_clf = DecisionTreeClassifier(random_state=0)
tree_clf.fit(X_train, y_train)

print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)

params = {
    "criterion":("gini", "entropy"),
    "splitter":("best", "random"),
    "max_depth":(list(range(1, 20))),
    "min_samples_split":[2, 3, 4],
    "min_samples_leaf":list(range(1, 20)),
}


tree_clf = DecisionTreeClassifier(random_state=0)
tree_cv = GridSearchCV(tree_clf, params, scoring="accuracy", n_jobs=-1, verbose=1, cv=3)
tree_cv.fit(X_train, y_train)
best_params = tree_cv.best_params_
print(f"Best paramters: {best_params})")

tree_clf = DecisionTreeClassifier(**best_params)
tree_clf.fit(X_train, y_train)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)

decision_tree_classifier = DecisionTreeClassifier(criterion = 'gini', min_samples_split = 2, max_depth= 11, random_state=0)
decision_tree_classifier.fit(X_train, y_train)
predictions = decision_tree_classifier.predict(X_test)

cm = ConfusionMatrix(decision_tree_classifier)
cm.fit(X_train, y_train)
cm.score(X_test, y_test)

classification_decision_tree = (classification_report(y_test, predictions))
print(classification_decision_tree)

rf_clf = RandomForestClassifier(n_estimators=100)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators, 'max_features': max_features,
               'max_depth': max_depth, 'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}

rf_clf = RandomForestClassifier(random_state=0)

rf_cv = RandomizedSearchCV(estimator=rf_clf, scoring='f1',param_distributions=random_grid, n_iter=100, cv=3,
                               verbose=2, random_state=42, n_jobs=-1)

rf_cv.fit(X_train, y_train)
rf_best_params = rf_cv.best_params_
print(f"Best paramters: {rf_best_params})")

rf_clf = RandomForestClassifier(**rf_best_params)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

columns = data.drop('id', axis = 1).columns
feature_imp = pd.Series(decision_tree_classifier.feature_importances_, index = columns).sort_values(ascending = False)
feature_imp

k_values = [i for i in range (3,12)]
scores = []

scaler = StandardScaler()
X = scaler.fit_transform(X)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5)
    scores.append(np.mean(score))

sns.lineplot(x = k_values, y = scores, marker = 'o')
plt.xlabel("K Values")
plt.ylabel("Accuracy Score")

clf1 = setup(data = data,
             target = 'target',
             numeric_imputation = 'mean',
             categorical_features = ['keyword','location', 'text'],
             ignore_features = ['id'])

compare_models()

rf1  = create_model('rf')

tuned_rf = tune_model(rf1)

plot_model(estimator = tuned_rf, plot = 'learning')

plot_model(estimator = tuned_rf, plot = 'auc')

plot_model(estimator = tuned_rf, plot = 'confusion_matrix')

plot_model(estimator = tuned_rf, plot = 'feature')

import shap
columns = data.drop('id', axis = 1).columns
explainer = shap.TreeExplainer(decision_tree_classifier)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values,X_test,plot_type='bar',max_display=10,class_names=["1", "0"], feature_names = columns.tolist())

evaluate_model(tuned_rf)

# # Get the model explainer object
#explainer = shap.KernelExplainer(knn_classifier.predict_proba, X_train)

# # Get shap values for the test data observation whose index is 0, i.e. first observation in the test set
#shap_values = explainer.shap_values(X_test)

# # Generate a force plot for this first observation using the derived shap values
#shap.force_plot(explainer.expected_value[0], shap_values[0], X_test)

#import gradio as gra
#def user_greeting(name):
     #return "Hi! " + name + " Welcome to Project X!üòé"

## define gradio interface and other parameters
#app =  gra.Interface(fn = user_greeting, inputs="text", outputs="text")
#app.launch()